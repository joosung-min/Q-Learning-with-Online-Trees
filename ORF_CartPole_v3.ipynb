{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9-final"},"orig_nbformat":2,"kernelspec":{"name":"python_defaultSpec_1600761156653","display_name":"Python 3.7.9 64-bit ('torch': venv)"},"colab":{"name":"ORF_CartPole_v3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"PR32cfEM_vaT"},"source":["## Q-learning with ORF"]},{"cell_type":"markdown","metadata":{"id":"VrYRKbdr_vaU"},"source":["**Algorithm: Q-learning with shallow function approximator**\n","\n","---\n","\n","Initialize replay memory D to capacity N\n","\n","Initialize action-value function Q with random weights\n","\n","**for** episode = 1 to M **do**\n","\n","&nbsp;&nbsp;&nbsp;&nbsp; Initialize sequence $s_{1} = \\{x_{1}\\}$ and preprocessed sequence $\\phi_{1} = \\phi(s_{1})$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp; **for** t = 1 to T **do**\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Select $a_{t} = \\begin{cases} \\max_{a}Q(\\phi(s_{t}), a; \\theta)&\\text{with probability } 1-\\epsilon \\\\ \\text{random action }&\\text{with probability } \\epsilon \\end{cases}$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Execute action $a_{t}$ and observe reward $r_{t}$ and image $x_{t+1}$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $s_{t+1}=s_{t}$, and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Store transition ($\\phi_{t}, a_{t}, r_{t}, \\phi_{t+1}$) in D\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sample random minibatch of transitions ($\\phi_{j}, a_{j}, r_{j}, \\phi_{j+1}$) from D\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $y_{j} = \\begin{cases} r_{j}&\\text{for terminal } \\phi_{j+1} \\\\ r_{j} + \\gamma \\max_{a'} Q(\\phi_{j+1}, a'; \\theta)&\\text{for non-terminal } \\phi_{j+1} \\end{cases}$\n","\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fit the approximator with ($\\phi_{j}$,  $y_{j}$)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp; **end for**\n","\n","**end for**\n","\n","---\n","\n","s = state, \n","\n","a = current action, \n","\n","a' = action for the next state, \n","\n","$\\theta$ = parameters for the function approximator, \n","\n","$Q(s,a;\\theta)$: action-value function estimated by a function approximator\n","\n"]},{"cell_type":"code","metadata":{"id":"cn_ITG37_vaU","executionInfo":{"status":"ok","timestamp":1601473527569,"user_tz":-540,"elapsed":15622,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}},"outputId":"0267ab24-70cd-4e06-bf0b-9086e14941b5","colab":{"base_uri":"https://localhost:8080/","height":503}},"source":["# Changes in v3: \n","# Expand number of trees to maxTrees at episode = 100\n","!apt-get install pypy"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","The following additional packages will be installed:\n","  pypy-lib\n","Suggested packages:\n","  pypy-doc pypy-tk\n","The following NEW packages will be installed:\n","  pypy pypy-lib\n","0 upgraded, 2 newly installed, 0 to remove and 21 not upgraded.\n","Need to get 13.1 MB of archives.\n","After this operation, 84.6 MB of additional disk space will be used.\n","Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pypy-lib amd64 5.10.0+dfsg-3build2 [2,303 kB]\n","Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pypy amd64 5.10.0+dfsg-3build2 [10.8 MB]\n","Fetched 13.1 MB in 2s (6,311 kB/s)\n","Selecting previously unselected package pypy-lib:amd64.\n","(Reading database ... 144619 files and directories currently installed.)\n","Preparing to unpack .../pypy-lib_5.10.0+dfsg-3build2_amd64.deb ...\n","Unpacking pypy-lib:amd64 (5.10.0+dfsg-3build2) ...\n","Selecting previously unselected package pypy.\n","Preparing to unpack .../pypy_5.10.0+dfsg-3build2_amd64.deb ...\n","Unpacking pypy (5.10.0+dfsg-3build2) ...\n","Setting up pypy-lib:amd64 (5.10.0+dfsg-3build2) ...\n","Setting up pypy (5.10.0+dfsg-3build2) ...\n","running pypy rtupdate hooks for 5.10\n","running pypy post-rtupdate hooks for 5.10\n","Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"t9fHii0w_vaX","executionInfo":{"status":"ok","timestamp":1601474088001,"user_tz":-540,"elapsed":582,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["import gym\n","import random\n","import pickle\n","from collections import deque\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import os\n","# import os.path\n","# import copy\n","import numpy as np\n","import time\n","import datetime\n","# import multiprocessing\n","# from cProfile import Profile"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"TkBDCntz_vaZ","executionInfo":{"status":"ok","timestamp":1601474092262,"user_tz":-540,"elapsed":609,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}},"outputId":"f648dd1e-f75f-4a60-879a-c33b12c41eb2","colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My\\ Drive/ShallowQlearning\n","import ORF as ORF"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/My Drive/ShallowQlearning\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zpEoZnwc_vab","executionInfo":{"status":"ok","timestamp":1601474100036,"user_tz":-540,"elapsed":703,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["class ORF_DQN(): \n","    \n","    def __init__(self, n_state, n_action, replay_size, ORFparams):\n","        self.n_action = n_action\n","        self.a_model = {} # to build RF for each action\n","        self.a_params = {a: ORFparams for a in range(n_action)}\n","        self.isFit = False\n","        self.maxTrees = ORFparams['maxTrees']\n","    \n","    def predict(self, s):            \n","        # s: (4,) array (for cartpole)\n","        \n","        preds = [self.a_model[a].predict(s) for a in range(self.n_action)]\n","        # print(preds)\n","        return preds\n","\n","    def gen_epsilon_greedy_policy(self, epsilon, n_action):\n","        \n","        def policy_function(state):\n","            # state: (4,) array\n","            ran = \"_\"\n","            q_values =[0.0, 0.0]\n","            if np.random.uniform(0,1) < epsilon:\n","                ran = \"Random\"\n","                return([random.randint(0, n_action - 1), ran, q_values])\n","            else:\n","                if self.isFit == True:\n","                    ran = \"Model\"\n","                    q_values = self.predict(state) # (1,2) array\n","                    # print(q_values)\n","                else: \n","                    ran = \"Random_notFit\"\n","                    return([random.randint(0, n_action - 1), ran, q_values])\n","                    # print(\"passed random.randint\")\n","            return([np.argmax(q_values), ran, q_values])# int\n","        \n","        return policy_function\n","\n","\n","    # def expandForest(self, memory):\n","    #     for a in range(self.n_action):\n","    #         self.a_params[a]['xrng'] = ORF.dataRange([v[0] for v in memory if v[1] == a])\n","    #         lenFor = len(self.a_model[a].forest)\n","    #         for i in range(lenFor+1, self.maxTrees):\n","    #             self.a_model[a].forest[i] = ORF.ORT(self.a_params[a]) # build new empty trees\n","    \n","\n","    def replay(self, memory, replay_size, gamma, episode):\n","        \n","        if len(memory) == replay_size: # Build initial Forests\n","            \n","            for a in range(self.n_action):\n","                self.a_params[a]['xrng'] = ORF.dataRange([v[0] for v in memory if v[1] == a])\n","                self.a_model[a] = ORF.ORF(self.a_params[a]) # Fit initial RFs for each action            \n","\n","        if len(memory) >= replay_size: # When the memory size exceeds the replay_size, start updating the RFs            \n","            \n","            replay_data = random.sample(memory, replay_size) # replay_data consists of [state, action, next_state, reward, is_done]\n","            for state, action, next_state, reward, is_done in replay_data:\n","                \n","                q_values = self.predict(state) # (, n_actions)\n","                q_values[action] = reward + gamma * np.max(self.predict(next_state)) if is_done == False else -1000 * reward\n","                \n","                # Update the RF for the action taken\n","                xrng = ORF.dataRange([v[0] for v in replay_data if v[1] == action])\n","                self.a_model[action].update(state, q_values[action], xrng)    \n","            self.isFit = True\n","               \n","        if episode == 100: # expand the number of trees at episode 100            \n","            # expandForest(memory)\n","            for a in range(self.n_action):\n","                self.a_params[a]['xrng'] = ORF.dataRange([v[0] for v in memory if v[1] == a])\n","                lenFor = len(self.a_model[a].forest)\n","                for i in range(lenFor+1, self.maxTrees):\n","                    self.a_model[a].forest[i] = ORF.ORT(self.a_params[a]) # build new empty trees\n","        \n","            \n","            "],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"ctoIOv24_vad","executionInfo":{"status":"ok","timestamp":1601474106395,"user_tz":-540,"elapsed":606,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["def q_learning(env, estimator, n_episode, replay_size, gamma=1.0, epsilon=0.1, epsilon_decay=0.95):\n","    \n","    for episode in tqdm(range(n_episode)):\n","        policy = estimator.gen_epsilon_greedy_policy(epsilon, n_action)\n","        state = env.reset()\n","        is_done = False\n","        i = 0\n","        while not is_done:\n","            action, ran, pred = policy(state) # integer\n","            next_state, reward, is_done, _ = env.step(action)\n","            i += 1\n","            # next_state: 4x1 array (for cartpole)\n","            # reward: integer\n","            # is_done: bool (True/False)\n","            \n","            total_reward_episode[episode] += reward\n","            \n","            # ep[episode].append((i, state, ran, action))\n","            memory.append((state, action, next_state, reward, is_done))\n","            \n","            if is_done:\n","                break\n","            estimator.replay(memory, replay_size, gamma, episode)\n","            state = next_state\n","        epsilon = np.max([epsilon * epsilon_decay, 0.001])\n","        # print(epsilon)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjBc6ZNu_vaf","executionInfo":{"status":"ok","timestamp":1601474119095,"user_tz":-540,"elapsed":591,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# Initialization\n","env = gym.envs.make(\"CartPole-v1\")\n","n_state = env.observation_space.shape[0]\n","n_action = env.action_space.n\n","\n","memory = deque(maxlen=10000)\n","n_episode = 50\n","replay_size = 32\n","\n","ORFparams = {'minSamples': replay_size*2, 'minGain': 0.1, 'xrng': None, 'maxDepth': 15, 'numTrees': 5, 'maxTrees': 30} # numTrees -> 30 after 100 iters. 25 restarts\n","\n","dqn = ORF_DQN(n_state, n_action, replay_size, ORFparams) \n","\n","total_reward_episode = [0] * n_episode\n","\n","ep = {i: [] for i in range(n_episode)}\n","\n","QLparams = {'gamma' : 1.0, 'epsilon' : 0.5, 'epsilon_decay' : 0.99}"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"93ITBCIv_vah","executionInfo":{"status":"ok","timestamp":1601474134759,"user_tz":-540,"elapsed":649,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}},"outputId":"b0d59fa6-cdc1-4b5f-b783-96dd5b00c0c7","colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["# Run alg\n","\n","start = time.time()\n","\n","q_learning(env, dqn, n_episode, replay_size, gamma=QLparams['gamma'], epsilon=QLparams['epsilon'], epsilon_decay=QLparams['epsilon_decay']) # runs the alg\n","\n","end = time.time()\n","\n","duration = int(end - start)\n","\n","print(\"learning duration =\", duration, \" seconds\")\n","print(\"mean reward = \", np.mean(total_reward_episode))\n","print(\"max reward = \", max(total_reward_episode))"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"<stdin>\", line 5, in <module>\n","NameError: global name 'q_learning' is not defined\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"yLC5PSSFRzz2","executionInfo":{"status":"ok","timestamp":1601473920116,"user_tz":-540,"elapsed":624,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}},"outputId":"77f724bd-6e70-40ec-deaa-1f907e2e576c","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["%%pypy\n","import platform"],"execution_count":18,"outputs":[{"output_type":"stream","text":["PyPy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YMEjwWYx_vak","executionInfo":{"status":"aborted","timestamp":1601471148197,"user_tz":-540,"elapsed":1162977,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["backup_file_name = \"ORF_CartPole_\" + time.strftime(\"%y%m%d\") + \"_1\"\n","img_file = backup_file_name + \".jpg\"\n","plt.plot(total_reward_episode)\n","plt.title(\"(ORF) Total reward per episode\")\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Total reward\")\n","plt.hlines(195, xmin=0, xmax=n_episode, linestyles=\"dotted\", colors=\"gray\")\n","plt.show()\n","plt.savefig(fname = img_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Qfx5mQm_vam","executionInfo":{"status":"aborted","timestamp":1601471148199,"user_tz":-540,"elapsed":1162976,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# To back-up the work\n","backup_file = backup_file_name + \".p\"\n","backup_check = os.path.isfile(backup_file)\n","\n","myEnv = dict()\n","myEnv[\"t_r_e\"] = total_reward_episode\n","myEnv[\"duration\"] = duration\n","myEnv[\"episode_details\"] = ep\n","myEnv[\"ORFparams\"] = ORFparams\n","myEnv[\"QLparams\"] = QLparams\n","# myEnv[\"ORF_params\"] = params\n","\n","with open(backup_file, \"wb\") as file:\n","    pickle.dump(myEnv, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j76eO7bC_vao","executionInfo":{"status":"aborted","timestamp":1601471148200,"user_tz":-540,"elapsed":1162975,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# About Cartpole\n","    \"\"\"\n","    Description:\n","        A pole is attached by an un-actuated joint to a cart, which moves along\n","        a frictionless track. The pendulum starts upright, and the goal is to\n","        prevent it from falling over by increasing and reducing the cart's\n","        velocity.\n","    \n","    Source:\n","        This environment corresponds to the version of the cart-pole problem\n","        described by Barto, Sutton, and Anderson\n","    \n","    Observation:\n","        Type: Box(4)\n","        Num     Observation               Min                     Max\n","        0       Cart Position             -4.8                    4.8\n","        1       Cart Velocity             -Inf                    Inf\n","        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n","        3       Pole Angular Velocity     -Inf                    Inf\n","    \n","    Actions:\n","        Type: Discrete(2)\n","        Num   Action\n","        0     Push cart to the left\n","        1     Push cart to the right\n","        Note: The amount the velocity that is reduced or increased is not\n","        fixed; it depends on the angle the pole is pointing. This is because\n","        the center of gravity of the pole increases the amount of energy needed\n","        to move the cart underneath it\n","    \n","    Reward:\n","        Reward is 1 for every step taken, including the termination step\n","    \n","    Starting State:\n","        All observations are assigned a uniform random value in [-0.05..0.05]\n","    \n","    Episode Termination:\n","        Pole Angle is more than 12 degrees.\n","        Cart Position is more than 2.4 (center of the cart reaches the edge of\n","        the display).\n","        Episode length is greater than 200.\n","    \n","    Solved Requirements:\n","        Considered solved when the average return is greater than or equal to\n","        195.0 over 100 consecutive trials.\n","    \"\"\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":["outputPrepend"],"id":"0yBfRwqc_vaq","executionInfo":{"status":"aborted","timestamp":1601471148201,"user_tz":-540,"elapsed":1162969,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["import pstats\n","stats = pstats.Stats(\"profile_200913.pfl\")\n","stats.strip_dirs()\n","stats.sort_stats('cumulative')\n","stats.print_stats()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9x9UMPr_var","executionInfo":{"status":"aborted","timestamp":1601471148201,"user_tz":-540,"elapsed":1162966,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":[""],"execution_count":null,"outputs":[]}]}