{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599715907677",
   "display_name": "Python 3.7.8 64-bit ('torch': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning with ORF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm: Q-learning with shallow function approximator**\n",
    "\n",
    "---\n",
    "\n",
    "Initialize replay memory D to capacity N\n",
    "\n",
    "Initialize action-value function Q with random weights\n",
    "\n",
    "**for** episode = 1 to M **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Initialize sequence $s_{1} = \\{x_{1}\\}$ and preprocessed sequence $\\phi_{1} = \\phi(s_{1})$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **for** t = 1 to T **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Select $a_{t} = \\begin{cases} \\max_{a}Q(\\phi(s_{t}), a; \\theta)&\\text{with probability } 1-\\epsilon \\\\ \\text{random action }&\\text{with probability } \\epsilon \\end{cases}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Execute action $a_{t}$ and observe reward $r_{t}$ and image $x_{t+1}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $s_{t+1}=s_{t}$, and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Store transition ($\\phi_{t}, a_{t}, r_{t}, \\phi_{t+1}$) in D\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sample random minibatch of transitions ($\\phi_{j}, a_{j}, r_{j}, \\phi_{j+1}$) from D\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $y_{j} = \\begin{cases} r_{j}&\\text{for terminal } \\phi_{j+1} \\\\ r_{j} + \\gamma \\max_{a'} Q(\\phi_{j+1}, a'; \\theta)&\\text{for non-terminal } \\phi_{j+1} \\end{cases}$\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fit the approximator with ($\\phi_{j}$,  $y_{j}$)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end for**\n",
    "\n",
    "**end for**\n",
    "\n",
    "---\n",
    "\n",
    "s = state, \n",
    "\n",
    "a = current action, \n",
    "\n",
    "a' = action for the next state, \n",
    "\n",
    "$\\theta$ = parameters for the function approximator, \n",
    "\n",
    "$Q(s,a;\\theta)$: action-value function estimated by a function approximator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import pickle\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import os.path\n",
    "import copy\n",
    "from PIL import Image\n",
    "# from sklearn.multioutput import MultiOutputRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ORF as ORF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORF_DQN(): \n",
    "    \n",
    "    def __init__(self, n_state, n_action, replay_size):\n",
    "        self.n_action = n_action\n",
    "        self.a_model = {} # to build RF for each action\n",
    "        self.a_params = {a: {'minSamples': replay_size*2, 'minGain': 0.1, 'xrng': None, 'maxDepth': 30} for a in range(n_action)}\n",
    "        self.isFit = False\n",
    "    \n",
    "    def predict(self, s):            \n",
    "        # s: (4,) array (for cartpole)\n",
    "        \n",
    "        preds = [self.a_model[a].predict(s) for a in range(self.n_action)]\n",
    "        # print(preds)\n",
    "        return preds\n",
    "\n",
    "    def gen_epsilon_greedy_policy(self, epsilon, n_action):\n",
    "        \n",
    "        def policy_function(state):\n",
    "            # state: (4,) array\n",
    "            ran = \"_\"\n",
    "            q_values =[0.0, 0.0]\n",
    "            if np.random.uniform(0,1) < epsilon:\n",
    "                ran = \"Random\"\n",
    "                return([random.randint(0, n_action - 1), ran, q_values])\n",
    "            else:\n",
    "                if self.isFit == True:\n",
    "                    ran = \"Model\"\n",
    "                    q_values = self.predict(state) # (1,2) array\n",
    "                    # print(q_values)\n",
    "                else: \n",
    "                    ran = \"Random_notFit\"\n",
    "                    return([random.randint(0, n_action - 1), ran, q_values])\n",
    "                    # print(\"passed random.randint\")\n",
    "            return([np.argmax(q_values), ran, q_values])# int\n",
    "        \n",
    "        return policy_function\n",
    "\n",
    "\n",
    "    def replay(self, memory, replay_size, gamma):\n",
    "        if len(memory) == replay_size: # Build initial models\n",
    "            \n",
    "            for a in range(self.n_action):\n",
    "                self.a_params[a]['xrng'] = ORF.dataRange([v[0] for v in memory if v[1] == a])\n",
    "                self.a_model[a] = ORF.ORF(self.a_params[a], numTrees=30) # Fit initial RFs for each action            \n",
    "            \n",
    "        if len(memory) >= replay_size: # When the memory size exceeds the replay_size, start updating the RFs\n",
    "            \n",
    "            replay_data = random.sample(memory, replay_size) # replay_data consists of [state, action, next_state, reward, is_done]\n",
    "            for state, action, next_state, reward, is_done in replay_data:\n",
    "                \n",
    "                q_values = self.predict(state) # (, n_actions)\n",
    "                q_values[action] = reward + gamma * np.max(self.predict(next_state)) if is_done == False else -1000 * reward\n",
    "                \n",
    "                # Update the RF for the action taken\n",
    "                xrng = ORF.dataRange([v[0] for v in replay_data if v[1] == action])\n",
    "                self.a_model[action].update(state, q_values[action])    \n",
    "            self.isFit = True\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, estimator, n_episode, replay_size, gamma=1.0, epsilon=0.1, epsilon_decay=0.95):\n",
    "    \n",
    "    for episode in tqdm(range(n_episode)):\n",
    "        policy = estimator.gen_epsilon_greedy_policy(epsilon, n_action)\n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "        i = 0\n",
    "        while not is_done:\n",
    "            action, ran, pred = policy(state) # integer\n",
    "            next_state, reward, is_done, _ = env.step(action)\n",
    "            i += 1\n",
    "            # next_state: 4x1 array (for cartpole)\n",
    "            # reward: integer\n",
    "            # is_done: bool (True/False)\n",
    "            \n",
    "            total_reward_episode[episode] += reward\n",
    "            \n",
    "            ep[episode].append((i, state, ran, action))\n",
    "            memory.append((state, action, next_state, reward, is_done))\n",
    "            \n",
    "            if is_done:\n",
    "                break\n",
    "            estimator.replay(memory, replay_size, gamma)\n",
    "            state = next_state\n",
    "        epsilon = np.max([epsilon * epsilon_decay, 0.01])\n",
    "        # print(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "20%|██        | 10/50 [09:29<36:07, 54.18s/it]Process ForkPoolWorker-20906:\nProcess ForkPoolWorker-20909:\nProcess ForkPoolWorker-20910:\nProcess ForkPoolWorker-20912:\n 20%|██        | 10/50 [09:32<38:09, 57.25s/it]Process ForkPoolWorker-20907:\nProcess ForkPoolWorker-20905:\nProcess ForkPoolWorker-20908:\nProcess ForkPoolWorker-20911:\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\nTraceback (most recent call last):\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\nTraceback (most recent call last):\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 297, in _bootstrap\n    self.run()\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\n  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 351, in get\n    with self._rlock:\nKeyboardInterrupt\n  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\n  File \"/usr/lib/python3.7/multiprocessing/synchronize.py\", line 95, in __enter__\n    return self._semlock.__enter__()\nKeyboardInterrupt\nKeyboardInterrupt\nKeyboardInterrupt\nKeyboardInterrupt\nKeyboardInterrupt\n  File \"/usr/lib/python3.7/multiprocessing/process.py\", line 99, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/lib/python3.7/multiprocessing/pool.py\", line 110, in worker\n    task = get()\n  File \"/usr/lib/python3.7/multiprocessing/queues.py\", line 352, in get\n    res = self._reader.recv_bytes()\n  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"/usr/lib/python3.7/multiprocessing/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nKeyboardInterrupt\n\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6fce87531455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_episode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# runs the alg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-7f924c50a37e>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, estimator, n_episode, replay_size, gamma, epsilon, epsilon_decay)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-94b1121af051>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, memory, replay_size, gamma)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# Update the RF for the action taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0mxrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mORF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataRange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreplay_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misFit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/joosu/Documents/Workspace/Git/ShallowQlearning/ORF.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtreeUpdate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m             \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mCLOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTERMINATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"In unknown state\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    557\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1042\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "backup_file_name = \"ORF_CartPole_\" + time.strftime(\"%y%m%d\") + \"_1\"\n",
    "\n",
    "env = gym.envs.make(\"CartPole-v1\")\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "\n",
    "memory = deque(maxlen=10000)\n",
    "n_episode = 50\n",
    "replay_size = 20\n",
    "\n",
    "dqn = ORF_DQN(n_state, n_action, replay_size) \n",
    "\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "ep = {i: [] for i in range(n_episode)}\n",
    "\n",
    "q_learning(env, dqn, n_episode, replay_size, gamma=1.0, epsilon=0.5, epsilon_decay=0.99) # runs the alg\n",
    "\n",
    "end = time.time()\n",
    "duration = int(end - start)\n",
    "\n",
    "img_file = backup_file_name + \".jpg\"\n",
    "print(\"learning duration =\", duration, \" seconds\")\n",
    "print(\"mean reward = \", np.mean(total_reward_episode))\n",
    "print(\"max reward = \", max(total_reward_episode))\n",
    "plt.plot(total_reward_episode)\n",
    "plt.title(\"(ORF) Total reward per episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.show()\n",
    "plt.savefig(fname = img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To back-up the work\n",
    "backup_file = backup_file_name + \".p\"\n",
    "backup_check = os.path.isfile(backup_file)\n",
    "\n",
    "myEnv = dict()\n",
    "myEnv[\"t_r_e\"] = total_reward_episode\n",
    "myEnv[\"duration\"] = duration\n",
    "myEnv[\"episode_details\"] = ep\n",
    "# myEnv[\"ORF_params\"] = params\n",
    "\n",
    "with open(backup_file, \"wb\") as file:\n",
    "    pickle.dump(myEnv, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}