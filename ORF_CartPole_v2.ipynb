{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599683179211",
   "display_name": "Python 3.7.8 64-bit ('torch': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning with ORF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm: Q-learning with shallow function approximator**\n",
    "\n",
    "---\n",
    "\n",
    "Initialize replay memory D to capacity N\n",
    "\n",
    "Initialize action-value function Q with random weights\n",
    "\n",
    "**for** episode = 1 to M **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Initialize sequence $s_{1} = \\{x_{1}\\}$ and preprocessed sequence $\\phi_{1} = \\phi(s_{1})$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **for** t = 1 to T **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Select $a_{t} = \\begin{cases} \\max_{a}Q(\\phi(s_{t}), a; \\theta)&\\text{with probability } 1-\\epsilon \\\\ \\text{random action }&\\text{with probability } \\epsilon \\end{cases}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Execute action $a_{t}$ and observe reward $r_{t}$ and image $x_{t+1}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $s_{t+1}=s_{t}$, and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Store transition ($\\phi_{t}, a_{t}, r_{t}, \\phi_{t+1}$) in D\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sample random minibatch of transitions ($\\phi_{j}, a_{j}, r_{j}, \\phi_{j+1}$) from D\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $y_{j} = \\begin{cases} r_{j}&\\text{for terminal } \\phi_{j+1} \\\\ r_{j} + \\gamma \\max_{a'} Q(\\phi_{j+1}, a'; \\theta)&\\text{for non-terminal } \\phi_{j+1} \\end{cases}$\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fit the approximator with ($\\phi_{j}$,  $y_{j}$)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end for**\n",
    "\n",
    "**end for**\n",
    "\n",
    "---\n",
    "\n",
    "s = state, \n",
    "\n",
    "a = current action, \n",
    "\n",
    "a' = action for the next state, \n",
    "\n",
    "$\\theta$ = parameters for the function approximator, \n",
    "\n",
    "$Q(s,a;\\theta)$: action-value function estimated by a function approximator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import pickle\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import copy\n",
    "from PIL import Image\n",
    "# from sklearn.multioutput import MultiOutputRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ORF as ORF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORF_DQN(): \n",
    "    \n",
    "    def __init__(self, n_state, n_action, replay_size):\n",
    "        self.n_action = n_action\n",
    "        self.a_model = {} # to build RF for each action\n",
    "        self.a_params = {a: {'minSamples': replay_size*2, 'minGain': 0.1, 'xrng': None, 'maxDepth': 30} for a in range(n_action)}\n",
    "        self.isFit = False\n",
    "    \n",
    "    def predict(self, s):            \n",
    "        # s: (4,) array (for cartpole)\n",
    "        \n",
    "        preds = [self.a_model[a].predict(s) for a in range(self.n_action)]\n",
    "        # print(preds)\n",
    "        return preds\n",
    "\n",
    "    def gen_epsilon_greedy_policy(self, epsilon, n_action):\n",
    "        \n",
    "        def policy_function(state):\n",
    "            # state: (4,) array\n",
    "            ran = \"_\"\n",
    "            q_values =[0.0, 0.0]\n",
    "            if np.random.random() < epsilon:\n",
    "                ran = \"Random\"\n",
    "                return([random.randint(0, n_action - 1), ran, q_values])\n",
    "            else:\n",
    "                if self.isFit == True:\n",
    "                    ran = \"Model\"\n",
    "                    q_values = self.predict(state) # (1,2) array\n",
    "                    # print(q_values)\n",
    "                else: \n",
    "                    ran = \"Random_notFit\"\n",
    "                    return([random.randint(0, n_action - 1), ran, q_values])\n",
    "                    # print(\"passed random.randint\")\n",
    "            return([np.argmax(q_values), ran, q_values])# int\n",
    "        \n",
    "        return policy_function\n",
    "\n",
    "\n",
    "    def replay(self, memory, replay_size, gamma):\n",
    "        if len(memory) == replay_size: # Build initial models\n",
    "            \n",
    "            for a in range(self.n_action):\n",
    "                self.a_params[a]['xrng'] = ORF.dataRange([v[0] for v in memory if v[1] == a])\n",
    "                self.a_model[a] = ORF.ORF(self.a_params[a], numTrees=30) # Fit initial RFs for each action            \n",
    "            \n",
    "        if len(memory) >= replay_size: # When the memory size exceeds the replay_size, start updating the RFs\n",
    "            \n",
    "            replay_data = random.sample(memory, replay_size) # replay_data consists of [state, action, next_state, reward, is_done]\n",
    "            for state, action, next_state, reward, is_done in replay_data:\n",
    "                \n",
    "                q_values = self.predict(state) # (, n_actions)\n",
    "                q_values[action] = reward + gamma * np.max(self.predict(next_state)) if is_done == False else -1000 * reward\n",
    "                \n",
    "                # Update the RF for the action taken\n",
    "                xrng = ORF.dataRange([v[0] for v in replay_data if v[1] == action])\n",
    "                self.a_model[action].update(state, q_values[action])    \n",
    "            self.isFit = True\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, estimator, n_episode, replay_size, gamma=1.0, epsilon=0.1, epsilon_decay=0.95):\n",
    "    \n",
    "    for episode in tqdm(range(n_episode)):\n",
    "        policy = estimator.gen_epsilon_greedy_policy(epsilon, n_action)\n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "        i = 0\n",
    "        while not is_done:\n",
    "            action, ran, pred = policy(state) # integer\n",
    "            next_state, reward, is_done, _ = env.step(action)\n",
    "            i += 1\n",
    "            # next_state: 4x1 array (for cartpole)\n",
    "            # reward: integer\n",
    "            # is_done: bool (True/False)\n",
    "            \n",
    "            total_reward_episode[episode] += reward\n",
    "            \n",
    "            ep[episode].append((i, state, ran, action))\n",
    "            memory.append((state, action, next_state, reward, is_done))\n",
    "            \n",
    "            if is_done:\n",
    "                break\n",
    "            estimator.replay(memory, replay_size, gamma)\n",
    "            state = next_state\n",
    "        epsilon = np.max([epsilon * epsilon_decay, 0.01])\n",
    "        # print(epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "49%|████▊     | 34/70 [21:08<22:23, 37.32s/it]\n"
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-1bda5253699c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# runs the alg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-74-7f924c50a37e>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, estimator, n_episode, replay_size, gamma, epsilon, epsilon_decay)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-167d1c5772d4>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, memory, replay_size, gamma)\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreplay_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (, n_actions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_done\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-167d1c5772d4>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# s: (4,) array (for cartpole)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# print(preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-167d1c5772d4>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# s: (4,) array (for cartpole)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;31m# print(preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/joosu/Documents/Workspace/Git/ShallowQlearning/ORF.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    483\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mORT\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# discard the tree and construct a new tree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 485\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    486\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m         \"\"\"\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/mnt/c/Users/joosu/Documents/Workspace/torch/lib/python3.7/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2239\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2241\u001b[0;31m     return _wrapreduction(a, np.add, 'sum', axis, dtype, out, keepdims=keepdims,\n\u001b[0m\u001b[1;32m   2242\u001b[0m                           initial=initial, where=where)\n\u001b[1;32m   2243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "backup_file_name = \"ORF_CartPole_\" + time.strftime(\"%y%m%d\") + \"_1\"\n",
    "\n",
    "env = gym.envs.make(\"CartPole-v1\")\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "\n",
    "memory = deque(maxlen=10000)\n",
    "n_episode = 70\n",
    "replay_size = 60\n",
    "\n",
    "dqn = ORF_DQN(n_state, n_action, replay_size) \n",
    "\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "ep = {}\n",
    "for i in range(n_episode):\n",
    "    ep[i] = []\n",
    "\n",
    "q_learning(env, dqn, n_episode, replay_size, gamma=1.0, epsilon=0.5, epsilon_decay=0.99) # runs the alg\n",
    "\n",
    "end = time.time()\n",
    "duration = int(end - start)\n",
    "\n",
    "img_file = backup_file_name + \".jpg\"\n",
    "print(\"learning duration =\", duration, \" seconds\")\n",
    "print(\"mean reward = \", np.mean(total_reward_episode))\n",
    "print(\"max reward = \", max(total_reward_episode))\n",
    "plt.plot(total_reward_episode)\n",
    "plt.title(\"(ORF) Total reward per episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.show()\n",
    "plt.savefig(fname = img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(1, array([-0.03281188, -0.03714004,  0.0163906 ,  0.03899407]), 'Random', 1),\n (2, array([-0.03355468,  0.15774308,  0.01717048, -0.24847273]), 'Model', 1),\n (3, array([-0.03039982,  0.35261566,  0.01220102, -0.53569064]), 'Model', 1),\n (4, array([-0.02334751,  0.54756394,  0.00148721, -0.82450428]), 'Model', 1),\n (5, array([-0.01239623,  0.74266552, -0.01500287, -1.11671908]), 'Model', 1),\n (6, array([ 0.00245708,  0.93798113, -0.03733726, -1.41407021]), 'Model', 0),\n (7, array([ 0.0212167 ,  0.74334118, -0.06561866, -1.13328836]), 'Model', 0),\n (8, array([ 0.03608353,  0.54913653, -0.08828443, -0.86188637]), 'Model', 0),\n (9, array([ 0.04706626,  0.35532049, -0.10552215, -0.59821646]), 'Random', 0),\n (10, array([ 0.05417267,  0.16182097, -0.11748648, -0.3405476 ]), 'Model', 0),\n (11, array([ 0.05740909, -0.03145048, -0.12429744, -0.08710056]), 'Model', 0),\n (12, array([ 0.05678008, -0.22459182, -0.12603945,  0.16392692]), 'Model', 0),\n (13,\n  array([ 0.05228824, -0.41770528, -0.12276091,  0.41434174]),\n  'Random',\n  1),\n (14,\n  array([ 0.04393414, -0.22107663, -0.11447407,  0.08561729]),\n  'Random',\n  0),\n (15, array([ 0.0395126 , -0.41438743, -0.11276173,  0.34010374]), 'Model', 0),\n (16, array([ 0.03122485, -0.60773946, -0.10595965,  0.59520811]), 'Model', 0),\n (17,\n  array([ 0.01907006, -0.80123133, -0.09405549,  0.85272425]),\n  'Random',\n  1),\n (18, array([ 0.00304544, -0.60496172, -0.07700101,  0.53200954]), 'Model', 1),\n (19, array([-0.0090538 , -0.40884598, -0.06636081,  0.21609166]), 'Model', 0),\n (20, array([-0.01723072, -0.60295958, -0.06203898,  0.4871254 ]), 'Model', 1),\n (21, array([-0.02928991, -0.40701964, -0.05229647,  0.17555316]), 'Model', 0),\n (22,\n  array([-0.0374303 , -0.60135567, -0.04878541,  0.45129052]),\n  'Random',\n  1),\n (23, array([-0.04945741, -0.40557893, -0.0397596 ,  0.14363758]), 'Model', 0),\n (24, array([-0.05756899, -0.60010958, -0.03688685,  0.42351671]), 'Model', 1),\n (25, array([-0.06957118, -0.40448502, -0.02841651,  0.1194372 ]), 'Model', 0),\n (26, array([-0.07766088, -0.59918855, -0.02602777,  0.40302125]), 'Model', 0),\n (27,\n  array([-0.08964466, -0.79393187, -0.01796735,  0.68738599]),\n  'Random',\n  1),\n (28,\n  array([-0.10552329, -0.59856519, -0.00421963,  0.38910116]),\n  'Random',\n  0),\n (29,\n  array([-0.1174946 , -0.79362699,  0.0035624 ,  0.68045069]),\n  'Random',\n  1),\n (30, array([-0.13336714, -0.5985547 ,  0.01717141,  0.38889146]), 'Model', 1),\n (31, array([-0.14533823, -0.40368064,  0.02494924,  0.1016716 ]), 'Model', 0),\n (32,\n  array([-0.15341184, -0.5991511 ,  0.02698267,  0.40212035]),\n  'Random',\n  1),\n (33,\n  array([-0.16539487, -0.40442204,  0.03502508,  0.11806504]),\n  'Random',\n  0),\n (34, array([-0.17348331, -0.60002787,  0.03738638,  0.42158909]), 'Model', 1),\n (35, array([-0.18548386, -0.405455  ,  0.04581816,  0.14092278]), 'Model', 0),\n (36,\n  array([-0.19359296, -0.6012022 ,  0.04863662,  0.44770126]),\n  'Random',\n  1),\n (37, array([-0.20561701, -0.40680078,  0.05759064,  0.17073749]), 'Model', 1),\n (38, array([-0.21375302, -0.21254839,  0.06100539, -0.10323573]), 'Model', 1),\n (39, array([-0.21800399, -0.01835136,  0.05894068, -0.37606493]), 'Model', 1),\n (40, array([-0.21837102,  0.17588606,  0.05141938, -0.6495966 ]), 'Model', 0),\n (41, array([-0.2148533 , -0.01991302,  0.03842745, -0.34117552]), 'Model', 1),\n (42, array([-0.21525156,  0.17464172,  0.03160394, -0.6214973 ]), 'Model', 0),\n (43,\n  array([-0.21175872, -0.02090698,  0.01917399, -0.31903071]),\n  'Random',\n  0),\n (44, array([-0.21217686, -0.21629669,  0.01279338, -0.02036322]), 'Model', 0),\n (45, array([-0.2165028 , -0.41159975,  0.01238611,  0.27632856]), 'Model', 1),\n (46,\n  array([-0.22473479, -0.21665668,  0.01791268, -0.01242219]),\n  'Random',\n  0),\n (47, array([-0.22906793, -0.41203088,  0.01766424,  0.28585814]), 'Model', 0),\n (48, array([-0.23730854, -0.60740024,  0.0233814 ,  0.58405948]), 'Model', 1),\n (49,\n  array([-0.24945655, -0.4126135 ,  0.03506259,  0.29883274]),\n  'Random',\n  1),\n (50,\n  array([-0.25770882, -0.21800843,  0.04103925,  0.01741084]),\n  'Random',\n  1),\n (51, array([-0.26206899, -0.02349833,  0.04138746, -0.2620465 ]), 'Model', 0),\n (52, array([-0.26253895, -0.21918588,  0.03614653,  0.0433979 ]), 'Model', 1),\n (53, array([-0.26692267, -0.0246004 ,  0.03701449, -0.2376649 ]), 'Model', 0),\n (54, array([-0.26741468, -0.22023106,  0.03226119,  0.06645988]), 'Model', 1),\n (55,\n  array([-0.2718193 , -0.02558614,  0.03359039, -0.21587243]),\n  'Random',\n  1),\n (56,\n  array([-0.27233102,  0.1690399 ,  0.02927294, -0.49777309]),\n  'Random',\n  0),\n (57,\n  array([-0.26895022, -0.0264823 ,  0.01931748, -0.19601043]),\n  'Random',\n  1),\n (58,\n  array([-0.26947987,  0.16835809,  0.01539727, -0.48253749]),\n  'Random',\n  0),\n (59,\n  array([-0.26611271, -0.02697776,  0.00574652, -0.18504183]),\n  'Random',\n  1),\n (60, array([-0.26665226,  0.1680615 ,  0.00204569, -0.47590642]), 'Model', 1),\n (61,\n  array([-0.26329103,  0.3631545 , -0.00747244, -0.76794389]),\n  'Random',\n  0),\n (62, array([-0.25602794,  0.16813621, -0.02283132, -0.47762149]), 'Model', 0),\n (63,\n  array([-0.25266522, -0.02665607, -0.03238375, -0.19222104]),\n  'Random',\n  0),\n (64,\n  array([-0.25319834, -0.22130015, -0.03622817,  0.09007284]),\n  'Random',\n  1),\n (65, array([-0.25762434, -0.02567816, -0.03442671, -0.21381637]), 'Model', 0),\n (66, array([-0.25813791, -0.22029143, -0.03870304,  0.06781112]), 'Model', 1),\n (67,\n  array([-0.26254374, -0.02463659, -0.03734682, -0.23682723]),\n  'Random',\n  1),\n (68, array([-0.26303647,  0.17099848, -0.04208336, -0.5410526 ]), 'Model', 0),\n (69,\n  array([-0.2596165 , -0.02350748, -0.05290442, -0.26192085]),\n  'Random',\n  0),\n (70,\n  array([-0.26008665, -0.21783589, -0.05814283,  0.0136173 ]),\n  'Random',\n  0),\n (71,\n  array([-0.26444337, -0.41207789, -0.05787049,  0.28740358]),\n  'Random',\n  1),\n (72, array([-0.27268492, -0.21618045, -0.05212241, -0.02295509]), 'Model', 1),\n (73,\n  array([-0.27700853, -0.02035126, -0.05258152, -0.33161701]),\n  'Random',\n  0),\n (74,\n  array([-0.27741556, -0.21468687, -0.05921386, -0.05596833]),\n  'Random',\n  0),\n (75,\n  array([-0.2817093 , -0.408912  , -0.06033322,  0.21746006]),\n  'Random',\n  1),\n (76, array([-0.28988754, -0.21298178, -0.05598402, -0.09362773]), 'Model', 0),\n (77, array([-0.29414717, -0.40725849, -0.05785658,  0.18088006]), 'Model', 0),\n (78,\n  array([-0.30229234, -0.60150686, -0.05423898,  0.45476475]),\n  'Random',\n  0),\n (79, array([-0.31432248, -0.79582162, -0.04514368,  0.72986982]), 'Model', 0),\n (80, array([-0.33023891, -0.9902915 , -0.03054628,  1.00800971]), 'Model', 0),\n (81, array([-0.35004474, -1.18499263, -0.01038609,  1.29094596]), 'Model', 0),\n (82,\n  array([-0.37374459, -1.37998099,  0.01543283,  1.58035926]),\n  'Random',\n  1),\n (83, array([-0.40134421, -1.18504613,  0.04704001,  1.29252886]), 'Model', 1),\n (84, array([-0.42504514, -0.99055263,  0.07289059,  1.01493599]), 'Model', 1),\n (85, array([-0.44485619, -0.79647447,  0.09318931,  0.74600263]), 'Model', 0),\n (86, array([-0.46078568, -0.99275028,  0.10810936,  1.06649691]), 'Model', 1),\n (87, array([-0.48064068, -0.79921186,  0.1294393 ,  0.80960641]), 'Model', 1),\n (88, array([-0.49662492, -0.6060785 ,  0.14563143,  0.56027886]), 'Model', 1),\n (89,\n  array([-0.50874649, -0.41326837,  0.15683701,  0.31678962]),\n  'Random',\n  0),\n (90,\n  array([-0.51701186, -0.61023599,  0.1631728 ,  0.65453602]),\n  'Random',\n  0),\n (91, array([-0.52921658, -0.80720851,  0.17626352,  0.99383038]), 'Model', 1),\n (92,\n  array([-0.54536075, -0.61482627,  0.19614013,  0.7612818 ]),\n  'Random',\n  1)]"
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "tr = [(i, v) for i, v in enumerate(total_reward_episode) if v <20]\n",
    "tr\n",
    "maxid = np.argmax(total_reward_episode)\n",
    "ep[43]\n",
    "ep[maxid]\n",
    "# minrw = np.argmin(total_reward_episode)\n",
    "# ep[minrw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To back-up the work\n",
    "backup_file = backup_file_name + \".p\"\n",
    "backup_check = os.path.isfile(backup_file)\n",
    "\n",
    "myEnv = dict()\n",
    "myEnv[\"t_r_e\"] = total_reward_episode\n",
    "myEnv[\"duration\"] = duration\n",
    "myEnv[\"episode_details\"] = ep\n",
    "# myEnv[\"ORF_params\"] = params\n",
    "\n",
    "with open(backup_file, \"wb\") as file:\n",
    "    pickle.dump(myEnv, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}