{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9-final"},"orig_nbformat":2,"kernelspec":{"name":"Python 3.7.9 64-bit ('python3')","display_name":"Python 3.7.9 64-bit ('python3')","metadata":{"interpreter":{"hash":"04464f91eb59e462538b6f3d1bb07a7fb2df3627af4e73aec650efd12f614edd"}}},"colab":{"name":"ORF_LunarLander_v1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"9T5wFr_OWnM8"},"source":["## Q-learning with ORF"]},{"cell_type":"markdown","metadata":{"id":"q-S7u7_WWnM9"},"source":["**Algorithm: Q-learning with shallow function approximator**\n","\n","---\n","\n","Initialize replay memory D to capacity N\n","\n","Initialize action-value function Q with random weights\n","\n","**for** episode = 1 to M **do**\n","\n","&nbsp;&nbsp;&nbsp;&nbsp; Initialize sequence $s_{1} = \\{x_{1}\\}$ and preprocessed sequence $\\phi_{1} = \\phi(s_{1})$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp; **for** t = 1 to T **do**\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Select $a_{t} = \\begin{cases} \\max_{a}Q(\\phi(s_{t}), a; \\theta)&\\text{with probability } 1-\\epsilon \\\\ \\text{random action }&\\text{with probability } \\epsilon \\end{cases}$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Execute action $a_{t}$ and observe reward $r_{t}$ and image $x_{t+1}$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $s_{t+1}=s_{t}$, and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Store transition ($\\phi_{t}, a_{t}, r_{t}, \\phi_{t+1}$) in D\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sample random minibatch of transitions ($\\phi_{j}, a_{j}, r_{j}, \\phi_{j+1}$) from D\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $y_{j} = \\begin{cases} r_{j}&\\text{for terminal } \\phi_{j+1} \\\\ r_{j} + \\gamma \\max_{a'} Q(\\phi_{j+1}, a'; \\theta)&\\text{for non-terminal } \\phi_{j+1} \\end{cases}$\n","\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fit the approximator with ($\\phi_{j}$,  $y_{j}$)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp; **end for**\n","\n","**end for**\n","\n","---\n","\n","s = state, \n","\n","a = current action, \n","\n","a' = action for the next state, \n","\n","$\\theta$ = parameters for the function approximator, \n","\n","$Q(s,a;\\theta)$: action-value function estimated by a function approximator\n","\n"]},{"cell_type":"code","metadata":{"id":"OhVaohq9WnM-","executionInfo":{"status":"ok","timestamp":1601473413707,"user_tz":-540,"elapsed":1143,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# Changes in v3: \n","# Expand number of trees to maxTrees at episode = 100\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1jZyeQ2WnNB","executionInfo":{"status":"ok","timestamp":1601473413708,"user_tz":-540,"elapsed":1135,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["import gym\n","import random\n","import pickle\n","from collections import deque\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import os\n","# import os.path\n","# import copy\n","import numpy as np\n","import time\n","import datetime\n","import sys\n","from utils import mean, argmax\n","# import multiprocessing\n","# from cProfile import Profile"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'gym'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-f255a6233246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeque\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"]}]},{"cell_type":"code","execution_count":17,"metadata":{"tags":[]},"outputs":[{"output_type":"stream","name":"stdout","text":["/mnt/c/Users/sonny/Gdrive/TreeQlearning/LunarLander\n"]}],"source":["print(os.getcwd())"]},{"cell_type":"code","metadata":{"id":"00weXqZp2Rzk","executionInfo":{"status":"ok","timestamp":1601473413708,"user_tz":-540,"elapsed":1128,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# # On Google Colab\n","# # # -------------------------------------------------------------------------\n","# from google.colab import drive\n","# drive.mount('/content/drive/')\n","# sys.path.insert(0, '/content/drive/My\\ Drive/Colab\\ Notebooks/')\n","# # # --------------------------------------------------------------------------\n","# !pip install box2d-py"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6_O333eGUjw","executionInfo":{"status":"ok","timestamp":1601473413709,"user_tz":-540,"elapsed":1121,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# !pip install gym[Box2D]"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZhpllB0P1GG","executionInfo":{"status":"ok","timestamp":1601473413709,"user_tz":-540,"elapsed":1105,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# !python setup_DQN.py build_ext --inplace"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"tVyrL26vPvuA","executionInfo":{"status":"ok","timestamp":1601473416528,"user_tz":-540,"elapsed":3914,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}},"outputId":"4bbe0b87-051b-4e1c-ea54-2ca1a7459f58","colab":{"base_uri":"https://localhost:8080/","height":53},"tags":[]},"source":["# !cython -a ORF_cython.pyx\n","# !cython -a DQN.pyx\n","# %cd /content/drive/My\\ Drive/ShallowQlearning/\n","# !python setup_ORF.py build_ext --inplace\n","# import pyximport\n","# pyximport.install()\n","# import ORF_cython as ORF\n","import ORF_py as ORF"],"execution_count":21,"outputs":[]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["class ORF_DQN: \n","    \n","    def __init__(self, n_state, n_action, replay_size, ORFparams):\n","        self.n_action = n_action\n","        self.a_model = {} # to build RF for each action\n","        self.a_params = {a: ORFparams for a in range(n_action)}\n","        self.isFit = False\n","        self.maxTrees = ORFparams['maxTrees']\n","    \n","    def predict(self, s):            \n","        # s: (4,) array (for cartpole)\n","        # preds = []\n","        # for a in range(self.n_action):\n","        #     preds.append(self.a_model[a].predict(s))\n","        \n","        preds = [self.a_model[a].predict(s) for a in range(self.n_action)]\n","        # print(preds)\n","        return preds\n","\n","    def gen_epsilon_greedy_policy(self, epsilon, n_action):\n","        \n","        def policy_function(state):\n","            # state: (4,) array\n","            ran = \"_\"\n","            q_values =[0.0, 0.0]\n","            if random.uniform(0,1) < epsilon:\n","                ran = \"Random\"\n","                return([random.randint(0, n_action - 1), ran, q_values])\n","            else:\n","                if self.isFit == True:\n","                    ran = \"Model\"\n","                    q_values = self.predict(state) # (1,2) array\n","                    # print(q_values)\n","                else: \n","                    ran = \"Random_notFit\"\n","                    return([random.randint(0, n_action - 1), ran, q_values])\n","                    # print(\"passed random.randint\")\n","            return([argmax(q_values), ran, q_values])# int\n","        \n","        return policy_function\n","\n","    def replay(self, memory, replay_size, gamma, episode):\n","\n","        if len(memory) == replay_size: # Build initial Forests\n","            \n","            for a in range(self.n_action):\n","                self.a_params[a]['xrng'] = ORF.dataRange([v[0] for v in memory if v[1] == a])\n","                self.a_model[a] = ORF.ORF(self.a_params[a]) # Fit initial RFs for each action            \n","\n","        if len(memory) >= replay_size: # When the memory size exceeds the replay_size, start updating the RFs            \n","            \n","            replay_data = random.sample(memory, replay_size) # replay_data consists of [state, action, next_state, reward, is_done]\n","            for state, action, next_state, reward, is_done in replay_data:\n","                \n","                q_values = self.predict(state) # (, n_actions)\n","                q_values[action] = reward + gamma * max(self.predict(next_state)) if is_done == False else -100*reward\n","                \n","                # Update the RF for the action taken\n","                xrng = ORF.dataRange([v[0] for v in replay_data if v[1] == action])\n","                self.a_model[action].update(state, q_values[action], xrng)    \n","            self.isFit = True\n","               \n","        if episode == 100: # expand the number of trees at episode 100            \n","            # expandForest(memory)\n","            for a in range(self.n_action):\n","                self.a_params[a]['xrng'] = ORF.dataRange([v[0] for v in memory if v[1] == a])\n","                lenFor = len(self.a_model[a].forest)\n","                for i in range(lenFor+1, self.maxTrees):\n","                    self.a_model[a].forest[i] = ORF.ORT(self.a_params[a]) # build new empty trees\n","\n","def q_learning(env, estimator, n_episode, replay_size, gamma=1.0, epsilon=0.1, epsilon_decay=0.95):\n","    \n","    for episode in tqdm(range(n_episode)):\n","        policy = estimator.gen_epsilon_greedy_policy(epsilon, n_action)\n","        state = env.reset()\n","        is_done = False\n","        i = 0\n","        while not is_done:\n","            action, ran, pred = policy(state) # integer\n","            next_state, reward, is_done, _ = env.step(action)\n","            i += 1\n","            # next_state: 4x1 array (for cartpole)\n","            # reward: integer\n","            # is_done: bool (True/False)\n","\n","            total_reward_episode[episode] += reward\n","            \n","            ## Modified rewards for LunarLander?\n","\n","            ep[episode].append((i, state, ran, action))\n","            # memory.append((state, action, next_state, modified_reward, is_done))\n","            memory.append((state, action, next_state, reward, is_done))\n","            \n","            if is_done:\n","                break\n","            estimator.replay(memory, replay_size, gamma, episode)\n","            state = next_state\n","        epsilon = max([epsilon * epsilon_decay, 0.01])\n","        # print(epsilon)"]},{"cell_type":"code","metadata":{"id":"sTMBXTFVWnNM","executionInfo":{"status":"ok","timestamp":1601473416530,"user_tz":-540,"elapsed":3886,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# Initialization\n","env = gym.envs.make(\"LunarLander-v2\")\n","# env=gym.envs.make(\"CartPole-v1\")\n","n_state = env.observation_space.shape[0]\n","n_action = env.action_space.n\n","memory = deque(maxlen=10000)\n","n_episode = 50\n","replay_size = 32\n","\n","ORFparams = {'minSamples': replay_size*2, 'minGain': 0.1, 'xrng': None, 'maxDepth': 15, 'numTrees': 5, 'maxTrees': 30} # numTrees -> 30 after 100 iters. 25 restarts\n","\n","dqn = ORF_DQN(n_state, n_action, replay_size, ORFparams) \n","\n","total_reward_episode = np.zeros(n_episode)\n","\n","ep = {i: [] for i in range(n_episode)}\n","\n","QLparams = {'gamma' : 1.0, 'epsilon' : 0.5, 'epsilon_decay' : 0.99}"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"BTfCbb3oWnNN","executionInfo":{"status":"error","timestamp":1601474802821,"user_tz":-540,"elapsed":1390168,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}},"outputId":"f4e84633-aa4c-499d-cd18-70f5ef6ce561","colab":{"base_uri":"https://localhost:8080/","height":502}},"source":["# Run alg\n","\n","start = time.time()\n","\n","q_learning(env, dqn, n_episode, replay_size, gamma=QLparams['gamma'], epsilon=QLparams['epsilon'], epsilon_decay=QLparams['epsilon_decay']) # runs the alg\n","\n","end = time.time()\n","\n","duration = int(end - start)\n","\n","print(\"learning duration =\", duration, \" seconds\")\n","print(\"mean reward = \", np.mean(total_reward_episode))\n","print(\"max reward = \", max(total_reward_episode))\n","\n"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["  2%|‚ñè         | 1/50 [00:19<15:41, 19.21s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-1821fa81eb32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQLparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQLparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epsilon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQLparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epsilon_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# runs the alg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-09b897171fed>\u001b[0m in \u001b[0;36mq_learning\u001b[0;34m(env, estimator, n_episode, replay_size, gamma, epsilon, epsilon_decay)\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-09b897171fed>\u001b[0m in \u001b[0;36mreplay\u001b[0;34m(self, memory, replay_size, gamma, episode)\u001b[0m\n\u001b[1;32m     53\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreplay_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (, n_actions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_done\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-09b897171fed>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#     preds.append(self.a_model[a].predict(s))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# print(preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-22-09b897171fed>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m#     preds.append(self.a_model[a].predict(s))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m# print(preds)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/mnt/c/Users/sonny/Gdrive/TreeQlearning/LunarLander/ORF_py.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0morf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \"\"\"\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# if regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/mnt/c/Users/sonny/Gdrive/TreeQlearning/LunarLander/ORF_py.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0morf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \"\"\"\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# if regression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/mnt/c/Users/sonny/Gdrive/TreeQlearning/LunarLander/ORF_py.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \"\"\"\n\u001b[0;32m--> 269\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__findLeaf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [0] returns the node, [1] returns the depth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__gains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/mnt/c/Users/sonny/Gdrive/TreeQlearning/LunarLander/ORF_py.py\u001b[0m in \u001b[0;36mpred\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# gives the predicted value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/mnt/c/Users/sonny/Gdrive/TreeQlearning/LunarLander/ORF_py.py\u001b[0m in \u001b[0;36mpred\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# gives predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__classify\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcounts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;31m# if self.__classify:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"PCSFVLYjWnNR","executionInfo":{"status":"aborted","timestamp":1601474802813,"user_tz":-540,"elapsed":1390138,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# backup_file_name = \"ORF_LunarLander_\" + time.strftime(\"%y%m%d\") + \"_1\"\n","backup_file_name = \"ORF_LunarLander_\" + time.strftime(\"%y%m%d\") + \"_1\"\n","img_file = backup_file_name + \".jpg\"\n","plt.plot(total_reward_episode)\n","plt.title(\"(ORF) Total reward per episode\")\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Total reward\")\n","plt.hlines(195, xmin=0, xmax=n_episode, linestyles=\"dotted\", colors=\"gray\")\n","plt.show()\n","plt.savefig(fname = img_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wYWW57FyWnNU","executionInfo":{"status":"aborted","timestamp":1601474802815,"user_tz":-540,"elapsed":1390131,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# To back-up the work\n","backup_file = backup_file_name + \".p\"\n","backup_check = os.path.isfile(backup_file)\n","\n","myEnv = dict()\n","myEnv[\"t_r_e\"] = result\n","myEnv[\"duration\"] = duration\n","myEnv[\"episode_details\"] = ep\n","myEnv[\"ORFparams\"] = ORFparams\n","myEnv[\"QLparams\"] = QLparams\n","# myEnv[\"ORF_params\"] = params\n","\n","with open(backup_file, \"wb\") as file:\n","    pickle.dump(myEnv, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nKKZbAreWnNV","executionInfo":{"status":"aborted","timestamp":1601474802816,"user_tz":-540,"elapsed":1390123,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# About LunarLander\n","\"\"\"\n","Rocket trajectory optimization is a classic topic in Optimal Control.\n","According to Pontryagin's maximum principle it's optimal to fire engine full throttle or\n","turn it off. That's the reason this environment is OK to have discreet actions (engine on or off).\n","The landing pad is always at coordinates (0,0). The coordinates are the first two numbers in the state vector.\n","Reward for moving from the top of the screen to the landing pad and zero speed is about 100..140 points.\n","If the lander moves away from the landing pad it loses reward. The episode finishes if the lander crashes or\n","comes to rest, receiving an additional -100 or +100 points. Each leg with ground contact is +10 points.\n","Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame.\n","Solved is 200 points.\n","\n","Landing outside the landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land\n","on its first attempt. Please see the source code for details.\n","To see a heuristic landing, run:\n","python gym/envs/box2d/lunar_lander.py\n","To play yourself, run:\n","python examples/agents/keyboard_agent.py LunarLander-v2\n","Created by Oleg Klimov. Licensed on the same terms as the rest of OpenAI Gym.\n","\n","observation_space = (x coord, y coord, x velocity, y velocity, lander angle, angular velocity, right-leg grounded, left-leg grounded)\n","action_space = do nothing(0), fire left engine(1), fire mian engine(2), fire right engine(3)\n","\"\"\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":["outputPrepend"],"id":"ybW_6Ap0WnNX","executionInfo":{"status":"aborted","timestamp":1601474802817,"user_tz":-540,"elapsed":1390117,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# import pstats\n","# stats = pstats.Stats(\"profile_200913.pfl\")\n","# stats.strip_dirs()\n","# stats.sort_stats('cumulative')\n","# stats.print_stats()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Gm28lXsWnNZ","executionInfo":{"status":"aborted","timestamp":1601474802819,"user_tz":-540,"elapsed":1390111,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":[""],"execution_count":null,"outputs":[]}]}