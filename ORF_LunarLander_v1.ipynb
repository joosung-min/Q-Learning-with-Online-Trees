{"nbformat":4,"nbformat_minor":0,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9-final"},"orig_nbformat":2,"kernelspec":{"name":"python_defaultSpec_1600770196376","display_name":"Python 3.7.9 64-bit ('torch': venv)"},"colab":{"name":"ORF_LunarLander_v1.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"9T5wFr_OWnM8"},"source":["## Q-learning with ORF"]},{"cell_type":"markdown","metadata":{"id":"q-S7u7_WWnM9"},"source":["**Algorithm: Q-learning with shallow function approximator**\n","\n","---\n","\n","Initialize replay memory D to capacity N\n","\n","Initialize action-value function Q with random weights\n","\n","**for** episode = 1 to M **do**\n","\n","&nbsp;&nbsp;&nbsp;&nbsp; Initialize sequence $s_{1} = \\{x_{1}\\}$ and preprocessed sequence $\\phi_{1} = \\phi(s_{1})$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp; **for** t = 1 to T **do**\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Select $a_{t} = \\begin{cases} \\max_{a}Q(\\phi(s_{t}), a; \\theta)&\\text{with probability } 1-\\epsilon \\\\ \\text{random action }&\\text{with probability } \\epsilon \\end{cases}$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Execute action $a_{t}$ and observe reward $r_{t}$ and image $x_{t+1}$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $s_{t+1}=s_{t}$, and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Store transition ($\\phi_{t}, a_{t}, r_{t}, \\phi_{t+1}$) in D\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sample random minibatch of transitions ($\\phi_{j}, a_{j}, r_{j}, \\phi_{j+1}$) from D\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $y_{j} = \\begin{cases} r_{j}&\\text{for terminal } \\phi_{j+1} \\\\ r_{j} + \\gamma \\max_{a'} Q(\\phi_{j+1}, a'; \\theta)&\\text{for non-terminal } \\phi_{j+1} \\end{cases}$\n","\n","\n","&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fit the approximator with ($\\phi_{j}$,  $y_{j}$)\n","\n","&nbsp;&nbsp;&nbsp;&nbsp; **end for**\n","\n","**end for**\n","\n","---\n","\n","s = state, \n","\n","a = current action, \n","\n","a' = action for the next state, \n","\n","$\\theta$ = parameters for the function approximator, \n","\n","$Q(s,a;\\theta)$: action-value function estimated by a function approximator\n","\n"]},{"cell_type":"code","metadata":{"id":"OhVaohq9WnM-","executionInfo":{"status":"ok","timestamp":1601473413707,"user_tz":-540,"elapsed":1143,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# Changes in v3: \n","# Expand number of trees to maxTrees at episode = 100\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1jZyeQ2WnNB","executionInfo":{"status":"ok","timestamp":1601473413708,"user_tz":-540,"elapsed":1135,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["import gym\n","import random\n","import pickle\n","from collections import deque\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import os\n","# import os.path\n","# import copy\n","import numpy as np\n","import time\n","import datetime\n","import sys\n","# import multiprocessing\n","# from cProfile import Profile"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"00weXqZp2Rzk","executionInfo":{"status":"ok","timestamp":1601473413708,"user_tz":-540,"elapsed":1128,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# # On Google Colab\n","# # # -------------------------------------------------------------------------\n","# from google.colab import drive\n","# drive.mount('/content/drive/')\n","# sys.path.insert(0, '/content/drive/My\\ Drive/Colab\\ Notebooks/')\n","# # # --------------------------------------------------------------------------\n","# !pip install box2d-py"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6_O333eGUjw","executionInfo":{"status":"ok","timestamp":1601473413709,"user_tz":-540,"elapsed":1121,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# !pip install gym[Box_2D]"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"BZhpllB0P1GG","executionInfo":{"status":"ok","timestamp":1601473413709,"user_tz":-540,"elapsed":1105,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# !python setup_DQN.py build_ext --inplace"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"tVyrL26vPvuA","executionInfo":{"status":"ok","timestamp":1601473416528,"user_tz":-540,"elapsed":3914,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}},"outputId":"4bbe0b87-051b-4e1c-ea54-2ca1a7459f58","colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["# !cython -a ORF_cython.pyx\n","# !cython -a DQN.pyx\n","# %cd /content/drive/My\\ Drive/ShallowQlearning/\n","# !python setup_ORF.py build_ext --inplace\n","import ORF_cython as ORF"],"execution_count":6,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/ShallowQlearning\n","running build_ext\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"sTMBXTFVWnNM","executionInfo":{"status":"ok","timestamp":1601473416530,"user_tz":-540,"elapsed":3886,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# Initialization\n","env = gym.envs.make(\"LunarLander-v2\")\n","# env=gym.envs.make(\"CartPole-v1\")\n","n_state = env.observation_space.shape[0]\n","n_action = env.action_space.n\n","memory = deque(maxlen=10000)\n","n_episode = 50\n","replay_size = 32\n","\n","ORFparams = {'minSamples': replay_size*2, 'minGain': 0.1, 'xrng': None, 'maxDepth': 15, 'numTrees': 5, 'maxTrees': 30} # numTrees -> 30 after 100 iters. 25 restarts\n","\n","dqn = ORF.ORF_DQN(n_state, n_action, replay_size, ORFparams) \n","\n","total_reward_episode = np.zeros(n_episode)\n","\n","ep = {i: [] for i in range(n_episode)}\n","\n","QLparams = {'gamma' : 1.0, 'epsilon' : 0.5, 'epsilon_decay' : 0.99}"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"id":"BTfCbb3oWnNN","executionInfo":{"status":"error","timestamp":1601474802821,"user_tz":-540,"elapsed":1390168,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}},"outputId":"f4e84633-aa4c-499d-cd18-70f5ef6ce561","colab":{"base_uri":"https://localhost:8080/","height":502}},"source":["# Run alg\n","\n","start = time.time()\n","\n","result = ORF.q_learning(env, dqn, n_episode, n_action, memory, replay_size, gamma=QLparams['gamma'], epsilon=QLparams['epsilon'], epsilon_decay=QLparams['epsilon_decay']) # runs the alg\n","\n","end = time.time()\n","\n","duration = int(end - start)\n","\n","print(\"learning duration =\", duration, \" seconds\")\n","print(\"mean reward = \", np.mean(total_reward_episode))\n","print(\"max reward = \", max(total_reward_episode))\n","\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":[" 58%|█████▊    | 29/50 [20:36<24:46, 70.76s/it]"],"name":"stderr"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-e982babae5b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mORF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdqn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_episode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQLparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gamma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQLparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epsilon'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQLparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epsilon_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# runs the alg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/ShallowQlearning/ORF_cython.pyx\u001b[0m in \u001b[0;36mORF_cython.q_learning\u001b[0;34m()\u001b[0m\n\u001b[1;32m    712\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplay_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/ShallowQlearning/ORF_cython.pyx\u001b[0m in \u001b[0;36mORF_cython.ORF_DQN.replay\u001b[0;34m()\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;31m#             self.a_model[a].forest[i] = ORF.ORT(self.a_params[a]) # build new empty trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m     \u001b[0mcpdef\u001b[0m \u001b[0mreplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m \u001b[0mreplay_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m \u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/ShallowQlearning/ORF_cython.pyx\u001b[0m in \u001b[0;36mORF_cython.ORF_DQN.replay\u001b[0;34m()\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;31m# Update the RF for the action taken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 679\u001b[0;31m                 \u001b[0mxrng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataRange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreplay_data\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxrng\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misFit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/ShallowQlearning/ORF_cython.pyx\u001b[0m in \u001b[0;36mORF_cython.ORF.update\u001b[0;34m()\u001b[0m\n\u001b[1;32m    471\u001b[0m         \u001b[0;31m# sequential updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# idx of trees with age > 1/gamma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m             \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# update each t in ORF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mage\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/My Drive/ShallowQlearning/ORF_cython.pyx\u001b[0m in \u001b[0;36mORF_cython.ORT.update\u001b[0;34m()\u001b[0m\n\u001b[1;32m    225\u001b[0m         \"\"\"\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# k = self.__poisson(1) # draw a random poisson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoisson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.poisson\u001b[0;34m()\u001b[0m\n","\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.int64_to_long\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36misscalar\u001b[0;34m(element)\u001b[0m\n\u001b[1;32m   1784\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1786\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0mset_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'numpy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m     \"\"\"\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"awxKdBVYnPGl"},"source":[""]},{"cell_type":"code","metadata":{"id":"PCSFVLYjWnNR","executionInfo":{"status":"aborted","timestamp":1601474802813,"user_tz":-540,"elapsed":1390138,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# backup_file_name = \"ORF_LunarLander_\" + time.strftime(\"%y%m%d\") + \"_1\"\n","backup_file_name = \"ORF_CartPole_\" + time.strftime(\"%y%m%d\") + \"_1\"\n","img_file = backup_file_name + \".jpg\"\n","plt.plot(total_reward_episode)\n","plt.title(\"(ORF) Total reward per episode\")\n","plt.xlabel(\"Episode\")\n","plt.ylabel(\"Total reward\")\n","plt.hlines(195, xmin=0, xmax=n_episode, linestyles=\"dotted\", colors=\"gray\")\n","plt.show()\n","plt.savefig(fname = img_file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wYWW57FyWnNU","executionInfo":{"status":"aborted","timestamp":1601474802815,"user_tz":-540,"elapsed":1390131,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# To back-up the work\n","backup_file = backup_file_name + \".p\"\n","backup_check = os.path.isfile(backup_file)\n","\n","myEnv = dict()\n","myEnv[\"t_r_e\"] = result\n","myEnv[\"duration\"] = duration\n","myEnv[\"episode_details\"] = ep\n","myEnv[\"ORFparams\"] = ORFparams\n","myEnv[\"QLparams\"] = QLparams\n","# myEnv[\"ORF_params\"] = params\n","\n","with open(backup_file, \"wb\") as file:\n","    pickle.dump(myEnv, file)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nKKZbAreWnNV","executionInfo":{"status":"aborted","timestamp":1601474802816,"user_tz":-540,"elapsed":1390123,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# About Cartpole\n","    \"\"\"\n","    Description:\n","        A pole is attached by an un-actuated joint to a cart, which moves along\n","        a frictionless track. The pendulum starts upright, and the goal is to\n","        prevent it from falling over by increasing and reducing the cart's\n","        velocity.\n","    \n","    Source:\n","        This environment corresponds to the version of the cart-pole problem\n","        described by Barto, Sutton, and Anderson\n","    \n","    Observation:\n","        Type: Box(4)\n","        Num     Observation               Min                     Max\n","        0       Cart Position             -4.8                    4.8\n","        1       Cart Velocity             -Inf                    Inf\n","        2       Pole Angle                -0.418 rad (-24 deg)    0.418 rad (24 deg)\n","        3       Pole Angular Velocity     -Inf                    Inf\n","    \n","    Actions:\n","        Type: Discrete(2)\n","        Num   Action\n","        0     Push cart to the left\n","        1     Push cart to the right\n","        Note: The amount the velocity that is reduced or increased is not\n","        fixed; it depends on the angle the pole is pointing. This is because\n","        the center of gravity of the pole increases the amount of energy needed\n","        to move the cart underneath it\n","    \n","    Reward:\n","        Reward is 1 for every step taken, including the termination step\n","    \n","    Starting State:\n","        All observations are assigned a uniform random value in [-0.05..0.05]\n","    \n","    Episode Termination:\n","        Pole Angle is more than 12 degrees.\n","        Cart Position is more than 2.4 (center of the cart reaches the edge of\n","        the display).\n","        Episode length is greater than 200.\n","    \n","    Solved Requirements:\n","        Considered solved when the average return is greater than or equal to\n","        195.0 over 100 consecutive trials.\n","    \"\"\"\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":["outputPrepend"],"id":"ybW_6Ap0WnNX","executionInfo":{"status":"aborted","timestamp":1601474802817,"user_tz":-540,"elapsed":1390117,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":["# import pstats\n","# stats = pstats.Stats(\"profile_200913.pfl\")\n","# stats.strip_dirs()\n","# stats.sort_stats('cumulative')\n","# stats.print_stats()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Gm28lXsWnNZ","executionInfo":{"status":"aborted","timestamp":1601474802819,"user_tz":-540,"elapsed":1390111,"user":{"displayName":"Joosung Min","photoUrl":"","userId":"05103045991196039152"}}},"source":[""],"execution_count":null,"outputs":[]}]}