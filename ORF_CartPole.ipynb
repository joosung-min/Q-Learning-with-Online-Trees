{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1599320799944",
   "display_name": "Python 3.7.8 64-bit ('torch': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning with ORF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm: Q-learning with shallow function approximator**\n",
    "\n",
    "---\n",
    "\n",
    "Initialize replay memory D to capacity N\n",
    "\n",
    "Initialize action-value function Q with random weights\n",
    "\n",
    "**for** episode = 1 to M **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; Initialize sequence $s_{1} = \\{x_{1}\\}$ and preprocessed sequence $\\phi_{1} = \\phi(s_{1})$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **for** t = 1 to T **do**\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Select $a_{t} = \\begin{cases} \\max_{a}Q(\\phi(s_{t}), a; \\theta)&\\text{with probability } 1-\\epsilon \\\\ \\text{random action }&\\text{with probability } \\epsilon \\end{cases}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Execute action $a_{t}$ and observe reward $r_{t}$ and image $x_{t+1}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $s_{t+1}=s_{t}$, and preprocess $\\phi_{t+1} = \\phi(s_{t+1})$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Store transition ($\\phi_{t}, a_{t}, r_{t}, \\phi_{t+1}$) in D\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sample random minibatch of transitions ($\\phi_{j}, a_{j}, r_{j}, \\phi_{j+1}$) from D\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Set $y_{j} = \\begin{cases} r_{j}&\\text{for terminal } \\phi_{j+1} \\\\ r_{j} + \\gamma \\max_{a'} Q(\\phi_{j+1}, a'; \\theta)&\\text{for non-terminal } \\phi_{j+1} \\end{cases}$\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Fit the approximator with ($\\phi_{j}$,  $y_{j}$)\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp; **end for**\n",
    "\n",
    "**end for**\n",
    "\n",
    "---\n",
    "\n",
    "s = state, \n",
    "\n",
    "a = current action, \n",
    "\n",
    "a' = action for the next state, \n",
    "\n",
    "$\\theta$ = parameters for the function approximator, \n",
    "\n",
    "$Q(s,a;\\theta)$: action-value function estimated by a function approximator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import pickle\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os.path\n",
    "import copy\n",
    "from PIL import Image\n",
    "# from sklearn.multioutput import MultiOutputRegressor\n",
    "# from lightgbm import LGBMRegressor\n",
    "import numpy as np\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ORF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORF_DQN(): \n",
    "    \n",
    "    def __init__(self, n_state, n_action):\n",
    "        self.n_action = n_action\n",
    "        self.a_model = {} # to build RF for each action\n",
    "        self.a_state = {} # to contain state for each action\n",
    "        self.a_target = {} # to contain q_value for each action\n",
    "        self.a_params = {}\n",
    "        for a in range(n_action): # To contain separate data for each action\n",
    "            self.a_state[a] = []\n",
    "            self.a_target[a] = []\n",
    "        self.isFit = False\n",
    "        self.rg = [] # for xrng\n",
    "        for _ in range(n_action):\n",
    "            self.rg.append([0, 255])\n",
    "\n",
    "    def predict(self, s):            \n",
    "        # s: (4,) array (for cartpole)\n",
    "        # s = np.array(s).reshape(1,-1) # converts (4,) to (1,4) (2 dimensional)\n",
    "        preds = []\n",
    "        \n",
    "        for a in range(self.n_action):\n",
    "            preds.append(self.a_model[a].predicts([s])) # should be (, n_action)\n",
    "        \n",
    "        # print(np.argmax(preds))\n",
    "        return preds\n",
    "\n",
    "    def gen_epsilon_greedy_policy(self, epsilon, n_action):\n",
    "        def policy_function(state):\n",
    "            # state: (4,) array\n",
    "            if np.random.random() < epsilon:\n",
    "                return random.randint(0, n_action - 1) # int\n",
    "            else:\n",
    "                if self.isFit == True:\n",
    "                    q_values = self.predict(state) # (1,2) array\n",
    "                    # print(q_values)\n",
    "                else: \n",
    "                    return random.randint(0, n_action - 1) # int\n",
    "                    # print(\"passed random.randint\")\n",
    "            return np.argmax(q_values) # int\n",
    "        return policy_function\n",
    "\n",
    "\n",
    "    def replay(self, memory, replay_size, gamma):\n",
    "        if len(memory) == replay_size: # Fit the initial RFs when memory size == replay_size\n",
    "            print(len(memory))\n",
    "            replay_data = random.sample(memory, replay_size)\n",
    "            \n",
    "            for state, action, next_state, reward, is_done in replay_data:\n",
    "                self.a_state[action].append(state) # create separate dataset for each action\n",
    "                # self.a_target[action].append(0)\n",
    "            \n",
    "            for i in range(n_action):\n",
    "                self.a_params[i] = {'minSamples': 2, 'minGain': 0.1, 'xrng': ORF.dataRange(self.rg), 'maxDepth': 30}\n",
    "                self.a_model[i] = ORF.ORF(self.a_params[i], numTrees=30) # Fit initial RFs for each action            \n",
    "            \n",
    "            # for a in range(self.n_action):\n",
    "            #     for j in range(len(self.a_state[a])):\n",
    "            #         self.a_model[a].update(self.a_state[a][j], self.a_target[a][j])\n",
    "\n",
    "            self.isFit = True\n",
    "        \n",
    "        elif len(memory) > replay_size: # When the memory size exceeds the replay_size, start updating the RFs\n",
    "            replay_data = random.sample(memory, replay_size)\n",
    "            # replay_data = memory[len(memory)-1] # draw the latest input\n",
    "            # replay_data consists of [state, action, next_state, reward, is_done]\n",
    "            \n",
    "            # Compute q-values for each state and action\n",
    "            for state, action, next_state, reward, is_done in replay_data:\n",
    "            # state, action, next_state, reward, is_done = replay_data\n",
    "            \n",
    "                q_values = self.predict(state) # (, n_actions)\n",
    "                \n",
    "                if is_done == False:\n",
    "                    q_values_next = self.predict(next_state) # (1,n_action) array\n",
    "                    q_values[action] = reward + gamma * np.max(q_values_next) # int\n",
    "                else:\n",
    "                    q_values[action] = -1 * reward\n",
    "                # Update both RFs\n",
    "            \n",
    "                self.a_model[action].update(state, q_values[action])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, estimator, n_episode, replay_size, gamma=1.0, epsilon=0.1, epsilon_decay=0.99):\n",
    "    \n",
    "    for episode in tqdm(range(n_episode)):\n",
    "        policy = estimator.gen_epsilon_greedy_policy(epsilon, n_action)\n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "\n",
    "        while not is_done:\n",
    "            action = policy(state) # integer\n",
    "            \n",
    "            next_state, reward, is_done, _ = env.step(action)\n",
    "            \n",
    "            # next_state: 4x1 array (for cartpole)\n",
    "            # reward: integer\n",
    "            # is_done: bool (True/False)\n",
    "            \n",
    "            total_reward_episode[episode] += reward\n",
    "            # print(state, action, next_state, reward, is_done)\n",
    "            memory.append((state, action, next_state, reward, is_done))\n",
    "            \n",
    "            if is_done:\n",
    "                break\n",
    "            estimator.replay(memory, replay_size, gamma)\n",
    "            state = next_state\n",
    "        epsilon = np.max([epsilon * epsilon_decay, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "env = gym.envs.make(\"CartPole-v1\")\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "\n",
    "memory = deque(maxlen=1000)\n",
    "n_episode = 600\n",
    "replay_size = 60\n",
    "\n",
    "dqn = ORF_DQN(n_state, n_action) \n",
    "\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_learning(env, dqn, n_episode, replay_size, gamma=1.0, epsilon=0.5, epsilon_decay=0.99) # runs the alg\n",
    "\n",
    "end = time.time()\n",
    "duration = int(end - start)\n",
    "\n",
    "img_file = backup_file_name + \".png\"\n",
    "print(\"learning duration =\", duration, \" seconds\")\n",
    "print(\"mean reward = \", np.mean(total_reward_episode))\n",
    "print(\"max reward = \", max(total_reward_episode))\n",
    "plt.plot(total_reward_episode)\n",
    "plt.title(\"(ORF) Total reward per episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.show()\n",
    "plt.savefig(img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To back-up the work\n",
    "backup_file_name = \"ORF_CartPole_\" + time.strftime(\"%y%m%d\") + \"_1\"\n",
    "backup_file = backup_file_name + \".p\"\n",
    "backup_check = os.path.isfile(backup_file)\n",
    "\n",
    "myEnv = dict()\n",
    "myEnv[\"t_r_e\"] = total_reward_episode\n",
    "myEnv[\"duration\"] = duration\n",
    "myEnv[\"ORF_params\"] = params\n",
    "\n",
    "with open(backup_file, \"wb\") as file:\n",
    "    pickle.dump(myEnv, file)"
   ]
  }
 ]
}