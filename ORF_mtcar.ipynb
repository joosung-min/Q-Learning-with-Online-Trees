{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit ('pypy3')",
   "display_name": "Python 3.6.9 64-bit ('pypy3')",
   "metadata": {
    "interpreter": {
     "hash": "779a1f3dc23c2fca0a11a4a2dea1f036c98fa23cec0b5457cf9cc0f821beaebf"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import pickle\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import mean, argmax\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ORF_py as ORF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORF_DQN: \n",
    "    \n",
    "    def __init__(self, n_state, n_action, replay_size, ORFparams):\n",
    "        self.n_action = n_action\n",
    "        self.a_model = {} # to build RF for each action\n",
    "        self.a_params = {a: ORFparams for a in range(n_action)}\n",
    "        self.isFit = False\n",
    "        self.maxTrees = ORFparams['maxTrees']\n",
    "    \n",
    "    def predict(self, s):            \n",
    "        # s: (4,) array (for cartpole)\n",
    "        # preds = []\n",
    "        # for a in range(self.n_action):\n",
    "        #     preds.append(self.a_model[a].predict(s))\n",
    "        \n",
    "        preds = [self.a_model[a].predict(s) for a in range(self.n_action)]\n",
    "        print(preds)\n",
    "        return preds\n",
    "\n",
    "    def gen_epsilon_greedy_policy(self, epsilon, n_action):\n",
    "        \n",
    "        def policy_function(state):\n",
    "            # state: (4,) array\n",
    "            ran = \"_\"\n",
    "            q_values =[0.0, 0.0]\n",
    "            if random.uniform(0,1) < epsilon:\n",
    "                ran = \"Random\"\n",
    "                return([random.randint(0, n_action - 1), ran, q_values])\n",
    "            else:\n",
    "                if self.isFit == True:\n",
    "                    ran = \"Model\"\n",
    "                    q_values = self.predict(state) # (1,2) array\n",
    "                    # print(q_values)\n",
    "                else: \n",
    "                    ran = \"Random_notFit\"\n",
    "                    return([random.randint(0, n_action - 1), ran, q_values])\n",
    "                    # print(\"passed random.randint\")\n",
    "            return([argmax(q_values), ran, q_values])# int\n",
    "        \n",
    "        return policy_function\n",
    "\n",
    "    def replay(self, memory, replay_size, gamma, episode):\n",
    "\n",
    "        if len(memory) == replay_size: # Build initial Forests\n",
    "            \n",
    "            for a in range(self.n_action):\n",
    "                self.a_params[a]['xrng'] = ORF.dataRange([v[0] for v in memory if v[1] == a])\n",
    "                self.a_model[a] = ORF.ORF(self.a_params[a]) # Fit initial RFs for each action            \n",
    "\n",
    "        if len(memory) >= replay_size: # When the memory size exceeds the replay_size, start updating the RFs            \n",
    "            \n",
    "            replay_data = random.sample(memory, replay_size) # replay_data consists of [state, action, next_state, reward, is_done]\n",
    "            for state, action, next_state, reward, is_done in replay_data:\n",
    "                \n",
    "                q_values = self.predict(state) # (, n_actions)\n",
    "                q_values[action] = reward + gamma * max(self.predict(next_state)) if is_done == False else reward\n",
    "                \n",
    "                # Update the RF for the action taken\n",
    "                xrng = ORF.dataRange([v[0] for v in replay_data if v[1] == action])\n",
    "                self.a_model[action].update(state, q_values[action], xrng)    \n",
    "            self.isFit = True\n",
    "               \n",
    "        if episode == 100: # expand the number of trees at episode 100            \n",
    "            # expandForest(memory)\n",
    "            for a in range(self.n_action):\n",
    "                self.a_params[a]['xrng'] = ORF.dataRange([v[0] for v in memory if v[1] == a])\n",
    "                lenFor = len(self.a_model[a].forest)\n",
    "                for i in range(lenFor+1, self.maxTrees):\n",
    "                    self.a_model[a].forest[i] = ORF.ORT(self.a_params[a]) # build new empty trees\n",
    "\n",
    "def q_learning(env, estimator, n_episode, replay_size, gamma=1.0, epsilon=0.1, epsilon_decay=0.95):\n",
    "    \n",
    "    for episode in tqdm(range(n_episode)):\n",
    "        policy = estimator.gen_epsilon_greedy_policy(epsilon, n_action)\n",
    "        state = env.reset()\n",
    "        is_done = False\n",
    "        i = 0\n",
    "        while not is_done:\n",
    "            action, ran, pred = policy(state) # integer\n",
    "            next_state, reward, is_done, _ = env.step(action)\n",
    "            i += 1\n",
    "            # print(action)\n",
    "            # next_state: 4x1 array (for cartpole)\n",
    "            # reward: integer\n",
    "            # is_done: bool (True/False)\n",
    "\n",
    "            total_reward_episode[episode] += reward\n",
    "            \n",
    "            # Modified rewards for mtcar depending on its location \n",
    "            # assign larger reward for being close to the right side\n",
    "\n",
    "            modified_reward = next_state[0] + 0.5\n",
    "            if next_state[0] >= 0.5: \n",
    "                modified_reward += 100 \n",
    "            elif next_state[0] >= 0.25:\n",
    "                modified_reward += 20\n",
    "            elif next_state[0] >= 0.1:\n",
    "                modified_reward += 10\n",
    "            elif next_state[0] >= 0:\n",
    "                modified_reward += 5\n",
    "\n",
    "            ep[episode].append((i, state, ran, action))\n",
    "            memory.append((state, action, next_state, modified_reward, is_done))\n",
    "            \n",
    "            if is_done:\n",
    "                break\n",
    "            estimator.replay(memory, replay_size, gamma, episode)\n",
    "            state = next_state\n",
    "        epsilon = max([epsilon * epsilon_decay, 0.01])\n",
    "        # print(epsilon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "env = gym.envs.make(\"MountainCar-v0\")\n",
    "n_state = env.observation_space.shape[0]\n",
    "n_action = env.action_space.n\n",
    "\n",
    "memory = deque(maxlen=100000)\n",
    "n_episode = 50\n",
    "replay_size = 32\n",
    "\n",
    "ORFparams = {'minSamples': replay_size*4, 'minGain': 0.1, 'xrng': None, 'maxDepth': 50, 'numTrees': 5, 'maxTrees': 30} # numTrees -> 30 after 100 iters. 25 restarts\n",
    "\n",
    "dqn = ORF_DQN(n_state, n_action, replay_size, ORFparams) \n",
    "\n",
    "total_reward_episode = [0] * n_episode\n",
    "\n",
    "ep = {i: [] for i in range(n_episode)}\n",
    "\n",
    "QLparams = {'gamma' : 1.0, 'epsilon' : 1.0, 'epsilon_decay' : 0.99}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Run alg\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "q_learning(env, dqn, n_episode, replay_size, gamma=QLparams['gamma'], epsilon=QLparams['epsilon'], epsilon_decay=QLparams['epsilon_decay']) # runs the alg\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "duration = int(end - start)\n",
    "\n",
    "print(\"learning duration =\", duration, \" seconds\")\n",
    "print(\"mean reward = \", mean(total_reward_episode))\n",
    "print(\"max reward = \", max(total_reward_episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup_file_name = \"ORF_MountainCar_\" + time.strftime(\"%y%m%d\") + \"_1\"\n",
    "img_file = backup_file_name + \".jpg\"\n",
    "plt.plot(total_reward_episode)\n",
    "plt.title(\"(ORF) Total reward per episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total reward\")\n",
    "plt.hlines(195, xmin=0, xmax=n_episode, linestyles=\"dotted\", colors=\"gray\")\n",
    "plt.show()\n",
    "plt.savefig(fname = img_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# About MountainCar\n",
    "# Actions: discrete(3) 0:left, 1:no push, 2: push right\n",
    "# Reward: -1 for each time step, until the goal position of 0.5 is reached.\n",
    "# Starting state: Random position from -0.6 to 0.4 with no velocity\n",
    "# Episode termination: ends when the car reaches 0.5 position, or if 200 iterations are reached.\n",
    "# Solve requirement: get average reward of -110.0 over 100 consecutive trials.\n"
   ]
  }
 ]
}